{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "arranged-oliver",
   "metadata": {},
   "source": [
    "# Safety Experiments\n",
    "\n",
    "Safety can be roughly translated as the agent avoiding bad trajectories (state/action).\n",
    "\n",
    "We aim at improving the policy, so that the output of the safety function, i.e. how quantifiably safe\n",
    "is my current state and action pair, is as small as possible.\n",
    "\n",
    "**Specification of saftey requirements**\n",
    "\n",
    "For this experiments, the requirements have been specified in the gym environment itself. We have set bounds for the Lander. If the Lander is flying inside these\n",
    "pre-determined bounds, we say that the it acted safely. We quanity the risk of of the Lander by calculating the distance between the safety bounds and the coordinates of the Lander. In other words: the further away the Lander flies from the bounds, the unsafer we say it acted.\n",
    "\n",
    "The main issue with this approach is the heuristic nature of the specifications. In real world applications, these would not be easy to find and may change anyway. \n",
    "\n",
    "**Approach and artefacts**\n",
    "\n",
    "+ Directly improve the agent\n",
    "    + Optimization methods\n",
    "    + Black box falidation methods\n",
    "+ Seperate controller\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-olympus",
   "metadata": {},
   "source": [
    "**Ressources**\n",
    "+ https://rlss.inria.fr/files/2019/07/SafeRL_tutorial.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-input",
   "metadata": {},
   "source": [
    "not covered:\n",
    "+ returning to save state\n",
    "+ learning a dynamics model, everything here is model-free\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-collectible",
   "metadata": {},
   "source": [
    "## ToDo\n",
    "\n",
    "### Importance sampling with CEM (off-policy)\n",
    "+ https://towardsdatascience.com/solving-a-reinforcement-learning-problem-using-cross-entropy-method-23d9726a737\n",
    "+ https://towardsdatascience.com/cross-entropy-method-for-reinforcement-learning-2b6de2a4f3a0\n",
    "\n",
    "### Optimization with gaussian process (on-policy)\n",
    "+ https://distill.pub/2019/visual-exploration-gaussian-processes/\n",
    "+ https://distill.pub/2020/bayesian-optimization/\n",
    "\n",
    "### Safety controller \n",
    "+ https://arxiv.org/pdf/1801.08757.pdf \n",
    "+ https://arxiv.org/pdf/2006.12136.pdf\n",
    "\n",
    "### Black Box Validation: Advesary\n",
    "+ https://ai.stanford.edu/blog/black-box-safety-validation/\n",
    "+ https://arxiv.org/pdf/2005.02979.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "outstanding-beauty",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Disable annoying tf2 warnings\n",
    "from warnings import simplefilter \n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "import gym\n",
    "import gym_safety\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "joint-salad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, env, state_size, action_size):\n",
    "        self.env = env\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # These are hyper parameters for the DQN\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.9999\n",
    "        self.epsilon_min = 0.001\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 1000\n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "\n",
    "        # initialize target model\n",
    "        self.update_target_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(64, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(self.action_size, activation='linear', kernel_initializer='he_uniform'))\n",
    "        #model.summary()\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def save(self, filename):\n",
    "        self.model.save_weights(filename)\n",
    "        \n",
    "    def load(self, filename):\n",
    "        self.model.load_weights(filename)\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(state)\n",
    "            return np.argmax(q_value[0])\n",
    "\n",
    "    def append_sample(self, state, action, reward, next_state, done, bounds):\n",
    "        self.memory.append((state, action, reward, next_state, done, bounds))\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def update_network(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        \n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        update_input = np.zeros((batch_size, self.state_size))\n",
    "        update_target = np.zeros((batch_size, self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            update_input[i] = mini_batch[i][0]\n",
    "            action.append(mini_batch[i][1])\n",
    "            reward.append(mini_batch[i][2])\n",
    "            update_target[i] = mini_batch[i][3]\n",
    "            done.append(mini_batch[i][4])\n",
    "\n",
    "        target = self.model.predict(update_input)\n",
    "        target_val = self.target_model.predict(update_target)\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            # Q Learning: get maximum Q value at s' from target model\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                target[i][action[i]] = reward[i] + self.discount_factor * (np.amax(target_val[i]))\n",
    "    \n",
    "        \n",
    "        \n",
    "        self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)\n",
    "        \n",
    "    \n",
    "    def train_agent(self, episodes=1000, render=False):\n",
    "        scores = []\n",
    "        for e in range(episodes):\n",
    "            done = False\n",
    "            score = 0\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state[:-2], [1, self.state_size])\n",
    "            while not done:\n",
    "                if render: self.env.render(mode='human')\n",
    "                # get action for the current state and go one step in environment\n",
    "                action = self.get_action(state)\n",
    "                next_state, reward, done, info = self.env.step(action)\n",
    "                bounds = np.reshape(next_state[-2:], [1, 2]) # seperate safety from other\n",
    "                next_state = np.reshape(next_state[:-2], [1, self.state_size])\n",
    "\n",
    "                self.append_sample(state, action, reward, next_state, done, bounds)\n",
    "\n",
    "                self.update_network()\n",
    "                score += reward\n",
    "                state = next_state\n",
    "\n",
    "                if done:\n",
    "                    # every episode update the target model to be same with model\n",
    "                    self.update_target_model()\n",
    "                    # every episode, plot the play time\n",
    "                    score = score if score == 500 else score + 100\n",
    "                    scores.append(score)\n",
    "                    print(f\"episode: {e}  | \" \\\n",
    "                          f\"score: {score}  | \" \\\n",
    "                          f\"memory: {len(self.memory)} | \" \\\n",
    "                          f\"epsilon: {self.epsilon}\")\n",
    "\n",
    "                    # if the mean of scores of last 10 episode is bigger than 490\n",
    "                    # stop training\n",
    "                    if np.mean(scores[-min(10, len(scores)):]) > 490:\n",
    "                        return\n",
    "        return scores\n",
    "\n",
    "    # pretty hacky but ok\n",
    "    def get_safe_bounds(self):\n",
    "        return [(i[0][0][-1], i[0][0][-2]) for i in self.memory]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "royal-hammer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reject_outliers(data, m=2):\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    distance = abs(data - mean)\n",
    "    not_outlier = distance < m * std\n",
    "    return data[not_outlier]\n",
    "\n",
    "def plot_visuals(agent, scores):\n",
    "    fig, axs = plt.subplots(ncols=2, figsize=(15, 5))\n",
    "    scores = reject_outliers(np.array(scores))\n",
    "    df = pd.DataFrame([(i, v) for i, v in enumerate(scores)], columns=[\"episode\", \"score\"])\n",
    "    sns.regplot(x=\"episode\", y=\"score\", data=df, robust=True, ci=None, scatter_kws={\"s\": 10}, ax=axs[0])\n",
    "    sns.boxplot(scores, showfliers=False, ax=axs[1])\n",
    "\n",
    "    bounds = bounds = agent.get_safe_bounds()\n",
    "    df = pd.DataFrame([(abs(v[0]-v[1])) for i, v in enumerate(bounds)], columns=[\"b\"])\n",
    "    sns.displot(df, x=\"b\", kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "muslim-scene",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing():\n",
    "    EPISODES = 15\n",
    "    env = gym.make('LunarSafe-v0')\n",
    "\n",
    "    # get size of state and action from environment\n",
    "    state_size = env.observation_space.shape[0] - 2\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    scores = agent.train_agent(episodes=EPISODES)\n",
    "    agent.save('dqn-lunar-safe-test.h2')\n",
    "    plot_visuals(agent, scores)\n",
    "    \n",
    "#testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dated-petite",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def training():\n",
    "    EPISODES = 300\n",
    "    env = gym.make('LunarSafe-v0')\n",
    "\n",
    "    # get size of state and action from environment\n",
    "    state_size = env.observation_space.shape[0] - 2\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    agent = DQNAgent(env, state_size, action_size)\n",
    "    scores = agent.train_agent(episodes=EPISODES)\n",
    "    agent.save('dqn-lunar-safe-trained.h2')    \n",
    "    plot_visuals(agent, scores)\n",
    "    \n",
    "#training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "contemporary-disclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():   \n",
    "    EPISODES = 15\n",
    "    env = gym.make('LunarSafe-v0')\n",
    "\n",
    "    state_size = env.observation_space.shape[0] - 2\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    agent = DQNAgent(env, state_size, action_size)\n",
    "    agent.load('dqn-lunar-safe-trained.h2')\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state[:-2], [1, state_size])\n",
    "        while not done:\n",
    "            env.render(mode='human')\n",
    "            action = agent.get_action(state)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            \n",
    "\n",
    "evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "safety-basic",
   "language": "python",
   "name": "safety-basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
